{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:16.842753700Z",
     "start_time": "2024-12-26T19:54:16.751822Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "#from keras import ops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "#from datasets import load_dataset\n",
    "from collections import Counter\n",
    "# from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.059763700Z",
     "start_time": "2024-12-26T19:54:16.855755600Z"
    }
   },
   "id": "13e8d6c7b3d1f0ec",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0327f965fb0992",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.267358500Z",
     "start_time": "2024-12-26T19:54:17.074962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f137a10",
   "metadata": {},
   "source": [
    "## Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43eec40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.382065Z",
     "start_time": "2024-12-26T19:54:17.322244Z"
    }
   },
   "outputs": [],
   "source": [
    "class ops:\n",
    "    @staticmethod\n",
    "    def shape(tensor):\n",
    "        return tf.shape(tensor)\n",
    "\n",
    "    @staticmethod\n",
    "    def arange(start, stop, step=1):\n",
    "        return tf.range(start, stop, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5ee40",
   "metadata": {},
   "source": [
    "## Build layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d58ae",
   "metadata": {},
   "source": [
    "Let's start by defining a TransformerBlock layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e7841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.526964700Z",
     "start_time": "2024-12-26T19:54:17.396026200Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489a114",
   "metadata": {},
   "source": [
    "Next, let's define a TokenAndPositionEmbedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6801187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.650984500Z",
     "start_time": "2024-12-26T19:54:17.592514900Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8e585",
   "metadata": {},
   "source": [
    "### Build the NER model class as a keras.Model subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcf3749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:17.787983200Z",
     "start_time": "2024-12-26T19:54:17.741857400Z"
    }
   },
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f2a71",
   "metadata": {},
   "source": [
    "## Make the NER label lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32884fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:18.030773100Z",
     "start_time": "2024-12-26T19:54:17.885762200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-EVE', 9: 'I-EVE', 10: 'B-NUM', 11: 'I-NUM', 12: 'B-MON', 13: 'I-MON', 14: 'B-LAN', 15: 'I-LAN', 16: 'B-TIME', 17: 'I-TIME'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    \n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"EVE\",'NUM','MON','LAN','TIME']\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ccb62",
   "metadata": {},
   "source": [
    "Get a list of all tokens in the training dataset. This will be used to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96eefc3-8af7-459a-b7c9-88b591d42191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:14:40.268012700Z",
     "start_time": "2024-12-26T20:14:40.133689200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "طµظˆط±ط© O\n",
      "\n",
      "ط¹ظ…ظ„ط© O\n",
      "\n",
      "ظˆط±ظ‚ظٹط© O\n",
      "\n",
      "ظ…ظ† O\n",
      "\n",
      "ظپط¦ط© O\n",
      "\n",
      "500 B-MONEY\n",
      "\n",
      "ظ…ظ„ط² I-MONEY\n",
      "\n",
      "ط®ظ„ط§ظ„ O\n",
      "\n",
      "ظپطھط±ط© O\n",
      "\n",
      "ط§ظ„ط§ظ†طھط¯ط§ط¨ B-EVENT\n",
      "\n",
      "ط§ظ„ط¨ط±ظٹط·ط§ظ†ظٹ I-EVENT\n",
      "\n",
      "ط¹ظ„ظ‰ I-EVENT\n",
      "\n",
      "ظپظ„ط³ط·ظٹظ† I-EVENT\n",
      "\n",
      ". O\n",
      "\n",
      " \n",
      "\n",
      "ط±ط³ط§ظ„ط© O\n",
      "\n",
      "ط§ظ„ط´ظٹط® O\n",
      "\n",
      "ط¹ط«ظ…ط§ظ† B-PERS\n",
      "\n",
      "ط²ظ‚ظˆطھ I-PERS\n",
      "\n",
      "ظپظٹ O\n"
     ]
    }
   ],
   "source": [
    "training_set_elements = []\n",
    "with open('C:/Users/TESTUSER/Desktop/UniversityCoursesFiles/uniYear4/AI/ArabicNamedEntityRecognition/code/data/train.txt') as my_file:\n",
    "    training_set_elements = my_file.readlines()\n",
    "\n",
    "for i in range(20):\n",
    "    print(training_set_elements[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa2367bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:54:27.770598700Z",
     "start_time": "2024-12-26T19:54:19.034684300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conll_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m all_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43mconll_data\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m], [])\n\u001B[0;32m      2\u001B[0m all_tokens_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mstr\u001B[39m\u001B[38;5;241m.\u001B[39mlower, all_tokens)))\n\u001B[0;32m      4\u001B[0m counter \u001B[38;5;241m=\u001B[39m Counter(all_tokens_array)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'conll_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
