{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m328s\u001B[0m 439ms/step - accuracy: 0.9731 - loss: 0.1468\n",
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('../backend')\n",
    "from assemblage import load_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, TimeDistributed, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_data(\"../data/train_cleaned.txt\")\n",
    "sentences, labels = zip(*data)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "X_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Padding\n",
    "max_length = max(len(seq) for seq in X_sequences)\n",
    "with open('../model/config.json', 'w') as config_file:\n",
    "    json.dump({'max_length': max_length}, config_file)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "unique_labels = set(label for seq in labels for label in seq)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(unique_labels))\n",
    "y_encoded = [[label_encoder.transform([l])[0] for l in seq] for seq in labels]\n",
    "y_padded = pad_sequences(y_encoded, maxlen=max_length, padding='post', value=label_encoder.transform(['O'])[0])\n",
    "y_categorical = [to_categorical(seq, num_classes=len(unique_labels)) for seq in y_padded]\n",
    "\n",
    "# Prepare the data\n",
    "X_train = X_padded\n",
    "y_train = np.array(y_categorical)\n",
    "\n",
    "# Define model\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(embedding_layer)\n",
    "output_layer = TimeDistributed(Dense(len(unique_labels), activation='softmax'))(bi_lstm)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=32, \n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the model in native Keras format\n",
    "model.save('../model/ner_model.keras')\n",
    "dump(tokenizer, '../model/tokenizer.joblib')\n",
    "dump(label_encoder, '../model/label_encoder.joblib')\n",
    "\n",
    "print(\"Model saved successfully\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T14:39:01.913380200Z",
     "start_time": "2024-12-27T14:32:48.828964900Z"
    }
   },
   "id": "238ade8516f21028"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
