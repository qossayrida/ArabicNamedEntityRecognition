{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2547507cbcb648"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import the data and prepare it for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1707c1634ddb22a9"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m103s\u001B[0m 136ms/step - accuracy: 0.9412 - loss: 0.2566 - val_accuracy: 0.9879 - val_loss: 0.0491\n",
      "Epoch 2/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m91s\u001B[0m 126ms/step - accuracy: 0.9901 - loss: 0.0377 - val_accuracy: 0.9915 - val_loss: 0.0333\n",
      "Epoch 3/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m88s\u001B[0m 122ms/step - accuracy: 0.9944 - loss: 0.0198 - val_accuracy: 0.9919 - val_loss: 0.0311\n",
      "Epoch 4/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m87s\u001B[0m 120ms/step - accuracy: 0.9964 - loss: 0.0123 - val_accuracy: 0.9925 - val_loss: 0.0310\n",
      "Epoch 5/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m86s\u001B[0m 118ms/step - accuracy: 0.9976 - loss: 0.0083 - val_accuracy: 0.9927 - val_loss: 0.0318\n",
      "Epoch 6/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m90s\u001B[0m 125ms/step - accuracy: 0.9984 - loss: 0.0058 - val_accuracy: 0.9927 - val_loss: 0.0340\n",
      "Epoch 7/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m90s\u001B[0m 124ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.9927 - val_loss: 0.0364\n",
      "Epoch 8/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m89s\u001B[0m 124ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 0.9926 - val_loss: 0.0370\n",
      "Epoch 9/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m97s\u001B[0m 134ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.9924 - val_loss: 0.0395\n",
      "Epoch 10/10\n",
      "\u001B[1m723/723\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m102s\u001B[0m 141ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9925 - val_loss: 0.0414\n",
      "\u001B[1m207/207\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - accuracy: 0.9942 - loss: 0.0316\n",
      "Test metrics:\n",
      "Test Loss: 0.04014710709452629\n",
      "Test Accuracy: 0.992664635181427\n",
      "\u001B[1m104/104\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - accuracy: 0.9939 - loss: 0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "Validation Loss: 0.04141910746693611\n",
      "Validation Accuracy: 0.9925218820571899\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                word, tag = line.strip().split()\n",
    "                sentence.append(word)\n",
    "                label.append(tag)\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "    return sentences, labels\n",
    "\n",
    "data, label_data = load_data(\"../data/train_cleaned.txt\")\n",
    "test_data, test_label_data = load_data(\"../data/test_cleaned.txt\")\n",
    "val_data, val_label_data = load_data(\"../data/val_cleaned.txt\")\n",
    "\n",
    "# Prepare vocabulary and labels\n",
    "all_words = list(set(word for sentence in data for word in sentence))\n",
    "all_tags = list(set(tag for tags in label_data for tag in tags))\n",
    "\n",
    "word2idx = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "label2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "idx2label = {idx: tag for tag, idx in label2idx.items()}\n",
    "\n",
    "# Convert data to indices\n",
    "def convert_to_indices(sentences, labels, word2idx, label2idx):\n",
    "    X = [[word2idx.get(word, word2idx[\"<UNK>\"]) for word in sentence] for sentence in sentences]\n",
    "    y = [[label2idx[tag] for tag in tags] for tags in labels]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_to_indices(data, label_data, word2idx, label2idx)\n",
    "X_val, y_val = convert_to_indices(val_data, val_label_data, word2idx, label2idx)\n",
    "X_test, y_test = convert_to_indices(test_data, test_label_data, word2idx, label2idx)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "y_train = pad_sequences(y_train, maxlen=max_len, padding=\"post\")\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\")\n",
    "y_val = pad_sequences(y_val, maxlen=max_len, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "y_test = pad_sequences(y_test, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=128, input_length=max_len),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True)),\n",
    "    TimeDistributed(Dense(len(label2idx), activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_val = np.expand_dims(y_val, -1)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_test = np.expand_dims(y_test, -1)\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"Test metrics:\")\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])\n",
    "\n",
    "results = model.evaluate(X_val, y_val)\n",
    "print(\"Validation metrics:\")\n",
    "print(\"Validation Loss:\", results[0])\n",
    "print(\"Validation Accuracy:\", results[1])\n",
    "\n",
    "# Save the model\n",
    "model.save(\"../model/arabic_ner_model.h5\")\n",
    "\n",
    "# Save mappings\n",
    "with open(\"../model/word2idx.pkl\", \"wb\") as file:\n",
    "    pickle.dump(word2idx, file)\n",
    "with open(\"../model/idx2label.pkl\", \"wb\") as file:\n",
    "    pickle.dump(idx2label, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T15:56:26.374452500Z",
     "start_time": "2024-12-31T15:40:56.471245700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 698ms/step\n",
      "Sentence: ['صورة', 'لعملة', 'ورقية', 'من', 'فئة', '500', 'ملز', 'خلال', 'فترة', 'الانتداب', 'البريطاني', 'على', 'فلسطين.']\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'B-MON', 'I-MON', 'O', 'O', 'B-EVE', 'I-EVE', 'I-EVE', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Load the model and make predictions\n",
    "loaded_model = tf.keras.models.load_model(\"../model/arabic_ner_model.h5\")\n",
    "\n",
    "# Load mappings\n",
    "with open(\"../model/word2idx.pkl\", \"rb\") as file:\n",
    "    word2idx = pickle.load(file)\n",
    "with open(\"../model/idx2label.pkl\", \"rb\") as file:\n",
    "    idx2label = pickle.load(file)\n",
    "\n",
    "# Prepare a sample sentence for prediction\n",
    "sample_sentence = \"صورة لعملة ورقية من فئة 500 ملز خلال فترة الانتداب البريطاني على فلسطين.\".split()\n",
    "sample_indices = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in sample_sentence]\n",
    "sample_padded = pad_sequences([sample_indices], maxlen=max_len, padding=\"post\")\n",
    "predictions = loaded_model.predict(sample_padded)\n",
    "predicted_tags = [idx2label[np.argmax(tag)] for tag in predictions[0]]\n",
    "\n",
    "# Print predictions\n",
    "print(\"Sentence:\", sample_sentence)\n",
    "print(\"Predicted Tags:\", predicted_tags[:len(sample_sentence)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T15:56:27.269816800Z",
     "start_time": "2024-12-31T15:56:26.384103600Z"
    }
   },
   "id": "130cae82e0cf7143"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
