{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np  # For this example, we use numpy for simplicity\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "#from keras import ops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "#from datasets import load_dataset\n",
    "from collections import Counter\n",
    "# from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b25a35c82c7500",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences, tags = [], []\n",
    "        sentence, tag = [], []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, label = line.rsplit(' ', 1)\n",
    "                sentence.append(word)\n",
    "                tag.append(label)\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                sentence, tag = [], []\n",
    "        if sentence:  # for the last sentence if there's no newline at the end\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag)\n",
    "    return sentences, tags\n",
    "\n",
    "# Reading the data\n",
    "training_set_path = 'C:/Users/TESTUSER/Desktop/UniversityCoursesFiles/uniYear4/AI/ArabicNamedEntityRecognition/code/data/train_cleaned.txt'\n",
    "validation_set_path = 'C:/Users/TESTUSER/Desktop/UniversityCoursesFiles/uniYear4/AI/ArabicNamedEntityRecognition/code/data/val_cleaned.txt'\n",
    "\n",
    "train_sentences, train_tags = read_data(training_set_path)\n",
    "val_sentences, val_tags = read_data(validation_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931c695f8e8ccd4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_tokens = [token for sentence in train_sentences + val_sentences for token in sentence]\n",
    "counter = Counter(all_tokens)\n",
    "vocab_size = 44561  # Adjust if needed based on your dataset size and memory constraints\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]  # -2 for [UNK] and [PAD]\n",
    "\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "tag_lookup = keras.layers.StringLookup(vocabulary=list(mapping.values()), num_oov_indices=0, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcccd5e26bb9844",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_data(sentences, tags):\n",
    "    token_ids = []\n",
    "    tag_ids = []\n",
    "    for sentence, tag_seq in zip(sentences, tags):\n",
    "        token_ids.append(lowercase_and_convert_to_ids(sentence))\n",
    "        tag_ids.append(tag_lookup(tag_seq))\n",
    "    return token_ids, tag_ids\n",
    "\n",
    "train_token_ids, train_tag_ids = process_data(train_sentences, train_tags)\n",
    "val_token_ids, val_tag_ids = process_data(val_sentences, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ecb0ab6955fcd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(token_ids, tag_ids, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((token_ids, tag_ids))\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=([None], [None]))\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_token_ids, train_tag_ids, batch_size)\n",
    "val_dataset = make_dataset(val_token_ids, val_tag_ids, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3df8c7235f08eb",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()  # For Arabic, you might want a more sophisticated tokenization\n",
    "    return lowercase_and_convert_to_ids(tf.constant(tokens))\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_text = \"صورة من فئة 500 مليون خلال فترة الانتداب البريطاني على فلسطين.\"\n",
    "sample_input = tokenize_and_convert_to_ids(sample_text)\n",
    "sample_input = ops.reshape(sample_input, shape=[1, -1])\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
