{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4948e0e-b498-4bce-a47e-1f53fb304b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow==2.16.1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:17:28.617743900Z",
     "start_time": "2024-12-26T23:17:28.561745800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from collections import Counter\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "import camel_tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0327f965fb0992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:17:15.747976200Z",
     "start_time": "2024-12-26T23:17:15.708791200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f137a10",
   "metadata": {},
   "source": [
    "## Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eec40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:12:59.673523800Z",
     "start_time": "2024-12-26T23:12:59.631274500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b5ee40",
   "metadata": {},
   "source": [
    "## Build layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d58ae",
   "metadata": {},
   "source": [
    "Let's start by defining a TransformerBlock layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717e7841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:12:59.676523900Z",
     "start_time": "2024-12-26T23:12:59.649384800Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489a114",
   "metadata": {},
   "source": [
    "Next, let's define a TokenAndPositionEmbedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6801187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:12:59.710955300Z",
     "start_time": "2024-12-26T23:12:59.663484400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8e585",
   "metadata": {},
   "source": [
    "### Build the NER model class as a keras.Model subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfcf3749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:12:59.713121600Z",
     "start_time": "2024-12-26T23:12:59.676523900Z"
    }
   },
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f2a71",
   "metadata": {},
   "source": [
    "## Make the NER label lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32884fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:12:59.714132300Z",
     "start_time": "2024-12-26T23:12:59.690068500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-EVE', 9: 'I-EVE', 10: 'B-NUM', 11: 'I-NUM', 12: 'B-MON', 13: 'I-MON', 14: 'B-LAN', 15: 'I-LAN', 16: 'B-TIME', 17: 'I-TIME'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"EVE\", 'NUM', 'MON', 'LAN', 'TIME']\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ccb62",
   "metadata": {},
   "source": [
    "Get a list of all tokens in the training dataset. This will be used to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c96eefc3-8af7-459a-b7c9-88b591d42191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:00.127784700Z",
     "start_time": "2024-12-26T23:12:59.707616400Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences, tags = [], []\n",
    "        sentence, tag = [], []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, label = line.rsplit(' ', 1)\n",
    "                sentence.append(word)\n",
    "                tag.append(label)\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append(tag)\n",
    "                sentence, tag = [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag)\n",
    "    return sentences, tags\n",
    "\n",
    "training_set_path = 'C:/Users/TESTUSER/Desktop/UniversityCoursesFiles/uniYear4/AI/ArabicNamedEntityRecognition/code/data/train_cleaned.txt'\n",
    "validation_set_path = 'C:/Users/TESTUSER/Desktop/UniversityCoursesFiles/uniYear4/AI/ArabicNamedEntityRecognition/code/data/val_cleaned.txt'\n",
    "\n",
    "train_sentences, train_tags = read_data(training_set_path)\n",
    "val_sentences, val_tags = read_data(validation_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae890c6fd6be8f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Tokenize sentences using CAMeL Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fb1fbd2-b99b-471f-8481-237ebff74690",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for sentence in train_sentences + val_sentences:\n",
    "    all_tokens.extend(simple_word_tokenize(' '.join(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa2367bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:00.687637900Z",
     "start_time": "2024-12-26T23:13:00.127784700Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter(all_tokens)\n",
    "vocab_size = 44561  # Adjust if needed\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]  # -2 for [UNK] and [PAD]\n",
    "\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "tag_lookup = keras.layers.StringLookup(vocabulary=list(mapping.values()), num_oov_indices=0, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a567b9-026b-4dee-96c2-59469642be52",
   "metadata": {},
   "source": [
    "### Process data for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c23c187bba3bee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:00.750647Z",
     "start_time": "2024-12-26T23:13:00.688895100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_data(sentences, tags):\n",
    "    token_ids = []\n",
    "    tag_ids = []\n",
    "    for sentence, tag_seq in zip(sentences, tags):\n",
    "        tokenized_sentence = simple_word_tokenize(' '.join(sentence))\n",
    "        token_ids.append(lookup_layer(tokenized_sentence))\n",
    "        tag_ids.append(tag_lookup(tag_seq))\n",
    "    return token_ids, tag_ids\n",
    "\n",
    "train_token_ids, train_tag_ids = process_data(train_sentences, train_tags)\n",
    "val_token_ids, val_tag_ids = process_data(val_sentences, val_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab8e84-b056-4520-a4e9-fa813041034b",
   "metadata": {},
   "source": [
    " ### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fadc9904990e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:01.263054800Z",
     "start_time": "2024-12-26T23:13:00.758164500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        padded = seq.numpy().tolist() + [0] * (max_len - len(seq))  # Assuming 0 is your padding token\n",
    "        padded_seqs.append(padded)\n",
    "    return tf.ragged.constant(padded_seqs).to_tensor()\n",
    "\n",
    "# Use this in your make_dataset function\n",
    "def make_dataset(token_ids, tag_ids, batch_size):\n",
    "    # Find the max length for both token_ids and tag_ids\n",
    "    max_len = max(max(len(seq) for seq in token_ids), max(len(seq) for seq in tag_ids))\n",
    "    \n",
    "    padded_token_ids = pad_sequences(token_ids, max_len)\n",
    "    padded_tag_ids = pad_sequences(tag_ids, max_len)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((padded_token_ids, padded_tag_ids))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_token_ids, train_tag_ids, batch_size)\n",
    "val_dataset = make_dataset(val_token_ids, val_tag_ids, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7ba05-44af-406c-a611-00d2a174d691",
   "metadata": {},
   "source": [
    "### Model compilation and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab57efa-8ad7-430d-9c32-b5d6ab5c8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 270) (32, 270)\n",
      "[[   65  6483 23781 ...     0     0     0]\n",
      " [   29   510  2863 ...     0     0     0]\n",
      " [   65   928  2798 ...     0     0     0]\n",
      " ...\n",
      " [  570   431   651 ...     0     0     0]\n",
      " [23800     8   663 ...     0     0     0]\n",
      " [  416  9391     7 ...     0     0     0]] [[1 1 1 ... 0 0 0]\n",
      " [1 1 2 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 2 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for tokens, tags in train_dataset.take(1):\n",
    "    print(tokens.shape, tags.shape)\n",
    "    print(tokens.numpy(), tags.numpy())  # Check if you see newline characters or unexpected patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ee22c7b845f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:01.280267500Z",
     "start_time": "2024-12-26T23:13:01.268859Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m445/723\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - loss: 1.7508"
     ]
    }
   ],
   "source": [
    "num_tags = len(mapping)\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, maxlen=512, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = ops.cast((y_true > 0), dtype=\"float32\")\n",
    "        loss = loss * mask\n",
    "        return ops.sum(loss) / ops.sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "#ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=loss)\n",
    "ner_model.fit(train_dataset, validation_data=val_dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e61ba7b4a9ea668c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T23:13:07.057839900Z",
     "start_time": "2024-12-26T23:13:01.283414600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape() got an unexpected keyword argument 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mصورة من فئة 500 مليون خلال فترة الانتداب البريطاني على فلسطين.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m tokenize_and_convert_to_ids(sample_text)\n\u001b[1;32m----> 7\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m ner_model\u001b[38;5;241m.\u001b[39mpredict(sample_input)\n\u001b[0;32m     10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: reshape() got an unexpected keyword argument 'shape'"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "# ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "# ner_model.fit(train_dataset, epochs=10)\n",
    "\n",
    "\n",
    "# def tokenize_and_convert_to_ids(text):\n",
    "#     tokens = text.split()\n",
    "#     return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "\n",
    "# # Sample inference using the trained model\n",
    "# sample_input = tokenize_and_convert_to_ids(\n",
    "#     \"eu rejects german call to boycott british lamb\"\n",
    "# )\n",
    "# sample_input = ops.reshape(sample_input, shape=[1, -1])\n",
    "# print(sample_input)\n",
    "\n",
    "# output = ner_model.predict(sample_input)\n",
    "# prediction = np.argmax(output, axis=-1)[0]\n",
    "# prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# # eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11e2a68c-70c3-4b47-ac78-2e2e9e8e02b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step\n",
      "['O', 'O', 'O', 'I-NUM', 'O', 'O', 'O', 'B-EVE', 'I-EVE', 'O', 'I-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "sample_text = \"صورة من فئة 500 مليون خلال فترة الانتداب البريطاني على فلسطين.\"\n",
    "sample_input = tokenize_and_convert_to_ids(sample_text)\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
